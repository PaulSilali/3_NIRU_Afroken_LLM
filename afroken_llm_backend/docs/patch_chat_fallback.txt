PATCH INSTRUCTIONS FOR: afroken_llm_backend/app/api/routes/chat.py

================================================================================
Add FAISS fallback when LLM_ENDPOINT and OPENAI_API_KEY are not set.
================================================================================

1. Add these imports at the top of chat.py (after existing imports):

   import os
   import json
   from pathlib import Path
   import numpy as np
   from app.embeddings_fallback import get_embedding

   # FAISS imports with fallback
   try:
       import faiss
       FAISS_AVAILABLE = True
   except ImportError:
       FAISS_AVAILABLE = False

2. Add this helper function before the router definition:

   def py_cosine_search(embeddings: np.ndarray, query_emb: np.ndarray, topk: int = 3):
       """Pure Python cosine similarity search (fallback if FAISS unavailable)."""
       query_norm = query_emb / (np.linalg.norm(query_emb) + 1e-8)
       emb_norms = embeddings / (np.linalg.norm(embeddings, axis=1, keepdims=True) + 1e-8)
       similarities = np.dot(emb_norms, query_norm)
       top_indices = np.argsort(-similarities)[:topk]
       top_distances = 1 - similarities[top_indices]
       return top_distances, top_indices

3. Replace the post_message function body with this code:

   @router.post("/messages", response_model=ChatResponse)
   async def post_message(req: ChatRequest):
       """
       Accept a user message, retrieve relevant documents, and generate an answer.
       
       If LLM_ENDPOINT and OPENAI_API_KEY are not set, returns top-k retrieved
       documents with excerpts instead of LLM-generated response.
       """
       
       from app.config import settings
       
       # Check if we should use FAISS fallback
       use_faiss_fallback = not settings.LLM_ENDPOINT and not os.getenv('OPENAI_API_KEY')
       
       if use_faiss_fallback:
           # FAISS fallback: return top-k documents
           try:
               backend_dir = Path(__file__).parent.parent.parent
               index_file = backend_dir / 'faiss_index.idx'
               doc_map_file = backend_dir / 'doc_map.json'
               
               if not index_file.exists() or not doc_map_file.exists():
                   return {
                       "reply": "RAG index not found. Please run the indexing pipeline first.",
                       "citations": []
                   }
               
               # Load doc_map
               with open(doc_map_file, 'r', encoding='utf-8') as f:
                   doc_map = json.load(f)
               
               # Get query embedding
               query_emb = get_embedding(req.message)
               
               # Load index and search
               if FAISS_AVAILABLE and index_file.exists():
                   index = faiss.read_index(str(index_file))
                   query_emb_32 = query_emb.astype('float32').reshape(1, -1)
                   distances, indices = index.search(query_emb_32, k=3)
                   top_indices = indices[0]
               else:
                   # Python fallback
                   embeddings_file = index_file.with_suffix('.npy')
                   if embeddings_file.exists():
                       embeddings = np.load(str(embeddings_file))
                       distances, top_indices = py_cosine_search(embeddings, query_emb, topk=3)
                   else:
                       return {
                           "reply": "RAG index not found. Please run the indexing pipeline first.",
                           "citations": []
                       }
               
               # Build answer from top-k documents
               answer_parts = []
               citations = []
               
               for idx in top_indices:
                   doc = doc_map[str(idx)]
                   title = doc.get('title', 'Untitled')
                   text = doc.get('text', '')
                   source = doc.get('source', '')
                   filename = doc.get('filename', '')
                   
                   # Add excerpt (first 200 chars)
                   excerpt = text[:200] + '...' if len(text) > 200 else text
                   answer_parts.append(f"**{title}**\n{excerpt}")
                   
                   citations.append({
                       "title": title,
                       "filename": filename,
                       "source": source
                   })
               
               # Combine answer (limit to 6000 chars)
               answer = "\n\n---\n\n".join(answer_parts)
               if len(answer) > 6000:
                   answer = answer[:6000] + "..."
               
               return {
                   "reply": answer,
                   "citations": citations
               }
               
           except Exception as e:
               return {
                   "reply": f"Error retrieving documents: {str(e)}",
                   "citations": []
               }
       
       # Original LLM flow (existing code)
       emb = await get_embedding(req.message)
       docs = vector_search(emb, top_k=5)
       
       context = "\n\n".join(
           [f"{d['title']}\n{d['content'][:1500]}" for d in docs]
       )
       
       if settings.LLM_ENDPOINT:
           payload = {
               "system": "You are AfroKen LLM. Answer in simple Swahili unless requested otherwise. Ground answers in provided documents and add citations.",
               "documents": context,
               "user_message": req.message,
               "language": req.language,
           }
           async with httpx.AsyncClient(timeout=20) as client:
               res = await client.post(settings.LLM_ENDPOINT, json=payload)
               res.raise_for_status()
               data = res.json()
               answer = data.get(
                   "answer", "Samahani, sijaelewa. Tafadhali fafanua."
               )
       else:
           answer = f"(demo) Nimepata {len(docs)} vyanzo. Jibu: {req.message}"
       
       citations = [d.get("source_url") or d.get("title") for d in docs]
       return {"reply": answer, "citations": citations}

================================================================================
NOTE: The chat route expects ChatRequest with 'message' field, not 'q'.
If your frontend sends 'q', update the schema or add compatibility.
================================================================================

